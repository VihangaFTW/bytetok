"""Regex-based byte-level tokenizer implementation."""

from enum import Enum
from .base_tok import Tokenizer
from ._bpe import Token
import regex as re
from pathlib import Path


class TokenPattern(str, Enum):
    """
    Pre-defined regex patterns for different tokenizer implementations.

    Sources:
    - GPT patterns: https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py
    - Other patterns: https://github.com/ggerganov/llama.cpp (llama-vocab.cpp)
    """

    # OpenAI models
    GPT2 = (
        r"'(?:[sdmt]|ll|ve|re)|"
        r" ?\p{L}+|"
        r" ?\p{N}+|"
        r" ?[^\s\p{L}\p{N}]+|"
        r"\s+(?!\S)|"
        r"\s+"
    )

    GPT4 = (
        r"'(?i:[sdmt]|ll|ve|re)|"
        r"[^\r\n\p{L}\p{N}]?+\p{L}+|"
        r"\p{N}{1,3}|"
        r" ?[^\s\p{L}\p{N}]++[\r\n]*|"
        r"\s*[\r\n]|"
        r"\s+(?!\S)|"
        r"\s+"
    )

    GPT4O = (
        r"[^\r\n\p{L}\p{N}]?((?=[\p{L}])([^a-z]))*((?=[\p{L}])([^A-Z]))+(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])?|"
        r"[^\r\n\p{L}\p{N}]?((?=[\p{L}])([^a-z]))+((?=[\p{L}])([^A-Z]))*(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])?|"
        r"\p{N}{1,3}|"
        r" ?[^\s\p{L}\p{N}]+[\r\n/]*|"
        r"\s*[\r\n]+|"
        r"\s+(?!\S)|"
        r"\s+"
    )

    # Meta models
    LLAMA3 = (
        r"(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])|"
        r"[^\r\n\p{L}\p{N}]?\p{L}+|"
        r"\p{N}{1,3}|"
        r" ?[^\s\p{L}\p{N}]+[\r\n]*|"
        r"\s*[\r\n]+|"
        r"\s+(?!\S)|"
        r"\s+"
    )

    # Alibaba models
    QWEN2 = (
        r"(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])|"
        r"[^\r\n\p{L}\p{N}]?\p{L}+|"
        r"\p{N}|"  # Single digits (different from LLAMA3)
        r" ?[^\s\p{L}\p{N}]+[\r\n]*|"
        r"\s*[\r\n]+|"
        r"\s+(?!\S)|"
        r"\s+"
    )

    # DeepSeek models
    DEEPSEEK_CODER = (
        r"[\r\n]|"
        r"\s?\p{L}+|"
        r"\s?\p{P}+|"
        r"[ä¸€-é¾¥à €-ä¸€ê°€-íŸ¿]+|"  # CJK characters
        r"\p{N}"
    )

    DEEPSEEK_LLM = (
        r"[\r\n]|"
        r"\s?[A-Za-zÂµÃ€-Ã–Ã˜-Ã¶Ã¸-ÆºÆ¼-Æ¿Ç„-Ê“Ê•-Ê¯Í°-Í³Í¶Í·Í»-Í½Í¿Î†Îˆ-ÎŠÎŒÎ-Î¡Î£-ÏµÏ·-ÒÒŠ-Ô¯Ô±-Õ–á‚ -áƒ…á -áµá¸-á½á²-á²ºá²½-á²¿á´€-á´«áµ«-áµ·áµ¹-á¶šá¸€-á¼•á¼˜-á¼á¼ -á½…á½ˆ-á½á½-á½—á½™á½›á½á½Ÿ-Ïá¾€-á¾´á¾¶-á¾¼Î¹á¿‚-á¿„á¿†-á¿Œá¿-Îá¿–-ÎŠá¿ -á¿¬á¿²-á¿´á¿¶-á¿¼â„‚â„‡â„Š-â„“â„•â„™-â„â„¤Î©â„¨K-â„­â„¯-â„´â„¹â„¼-â„¿â……-â…‰â…â†ƒâ†„â°€-â±»â±¾-â³¤â³«-â³®â³³â²€-â³¤â³«-â³®â³²â³³ê™€-ê™­êš€-êš›êœ¢-ê¯ê±-ê‡ê‹-êê­°-ê®¿ï¬€-ï¬†ï¬“-ï¬—ï¼¡-ï¼ºï½-ï½šğ€-ğ‘ğ’°-ğ““ğ“˜-ğ“»ğ²€-ğ²²ğ³€-ğ³²ğ‘¢ -ğ‘£Ÿğ¤€-ğ¥ƒ]+|"
        r"\s?[!-/:-~ï¼-ï¼ï¼š-ï½'-â€Ÿã€€-ã€‚]+|"
        r"\s+$|"
        r"[ä¸€-é¾¥à €-ä¸€ê°€-íŸ¿]+|"
        r"\p{N}+"
    )

    # coding-focused models
    STARCODER = (
        r"\p{N}|"
        r"'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|"
        r"\s+(?!\S)"
    )

    FALCON = (
        r"[\p{P}\$\+<=>^\~\|`]+|"
        r"'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|"
        r"\s+(?!\S)|"
        r"[0-9][0-9][0-9]"
    )

    # multilingual models
    BLOOM = r" ?[^(\s|.,!?â€¦ã€‚ï¼Œã€à¥¤Û”ØŒ)]+"

    CHATGLM4 = (
        r"(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])|"
        r"[^\r\n\p{L}\p{N}]?\p{L}+|"
        r"\p{N}{1,3}|"
        r" ?[^\s\p{L}\p{N}]+[\r\n]*|"
        r"\s*[\r\n]+|"
        r"\s+(?!\S)|"
        r"\s+"
    )

    @classmethod
    def get(cls, name: str) -> str:
        """Get patterns by name (case-insensitive)."""
        try:
            return cls[name.upper()].value
        except KeyError:
            raise ValueError(
                f"Unknown pattern: {name!r}. "
                f"Valid patterns: {', '.join(pat.name for pat in cls)}"
            )


class RegexTokenizer(Tokenizer):
    """Tokenizer that splits text using regex patterns before applying BPE."""

    def __init__(self, pattern: str = TokenPattern.GPT4.value) -> None:
        super().__init__()
        self.pattern = pattern

    def train(self, text, vocab_size, verbose=False):
        """Train tokenizer by learning byte pair merges on regex-split chunks."""
        pass

    def encode(self, text):
        """Encode text into a sequence of tokens using regex splitting."""
        pass

    def decode(self, tokens: list[Token]):
        """Decode a sequence of tokens back into text."""
        pass

    @classmethod
    def from_pattern(cls, pattern: str | TokenPattern) -> "RegexTokenizer":
        """
        Create tokenizer from a pattern name or custom pattern.

        Args:
            pattern: Either a TokenPattern enum member, pattern name string ("gpt2", "gpt4"),
                    or a custom regex pattern string.

        Returns:
            A new RegexTokenizer instance.

        Example:
            .. code-block:: python
            tokenizer = RegexTokenizer.from_pattern("gpt4")
            tokenizer = RegexTokenizer.from_pattern(TokenPattern.GPT2)
            tokenizer = RegexTokenizer.from_pattern(r"custom pattern")

        """

        if isinstance(pattern, TokenPattern):
            pat = pattern
        elif isinstance(pattern, str):
            try:
                # check for valid pattern
                pat = TokenPattern.get(pattern)
            except ValueError:
                # not a valid pattern name
                # verify custom regex compiles
                if _is_valid_regex(pattern):
                    pat = pattern
                else:
                    raise ValueError(f"Invalid regex: {pattern}")
        return cls(pattern=pat)

    @classmethod
    def from_pretrained(cls, model_path: str | Path) -> "RegexTokenizer":
        """
        Load a pre-trained tokenizer from a .model file.

        Args:
            model_path: Path to the .model file.

        Returns:
            A loaded RegexTokenizer instance.

        Raises:
            FileNotFoundError: If the model file doesn't exist.
            ValueError: If the file format is invalid.

        Example:
            .. code-block:: python
            tokenizer = RegexTokenizer.from_pretrained("path/to/model.model")
        """
        path = Path(model_path)

        if not path.exists():
            raise FileNotFoundError(f"Model file not found: {model_path}")

        if not path.suffix == ".model":
            raise ValueError(
                f"Expected .model file, got {path.suffix}. Path: {model_path}"
            )

        tokenizer = cls()
        tokenizer.load(str(model_path))

        return tokenizer


def _is_valid_regex(pattern: str) -> bool:
    try:
        re.compile(pattern)
        return True
    except re.error:
        return False
