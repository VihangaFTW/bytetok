"""Basic byte-level tokenizer implementation."""

from datasets import load_dataset
from ._bpe import Token, bpe_freqs, bpe_merge
from .base_tok import Tokenizer
import logging

log = logging.getLogger(__name__)


class BasicTokenizer(Tokenizer):
    """Tokenizer that operates directly on byte sequences without regex splitting."""

    def __init__(self) -> None:
        super().__init__()

    def train(self, text: list[int], vocab_size: int, verbose=False):
        """Train tokenizer by learning byte pair merges from the input sequence."""
        if vocab_size <= 256:
            raise ValueError("Vocab size must be greater than 256")
        # merges beyond base byte vocabulary
        n_merges = vocab_size - 256
        merges = {}
        vocab = {i: bytes([i]) for i in range(256)}
        # BPE algorithm
        for i in range(n_merges):
            # find most common token pair
            pairs = bpe_freqs(text)
            rank0 = pairs.most_common(1)[0][0]
            new_token = 256 + i
            # merge pair with new token
            text = bpe_merge(text, rank0, new_token)
            # save merge info and update vocabulary with new token's mapping
            merges[rank0] = new_token
            vocab[new_token] = vocab[rank0[0]] + vocab[rank0[1]]
            # debugging: log new merge info
            if verbose:
                log.info(f"Merge {i + 1}/{n_merges}: {rank0} -> {new_token}")

            self.enc_merges = merges  # used for encoding text -> tokens
            self.dec_vocab = vocab  # usef for decoding tokens -> text

    def encode(self, text: str) -> list[Token]:
        """Encode text into a sequence of tokens."""
        # encode Unicode text into bytes
        txt_bytes = text.encode("utf-8", errors="replace")
        # convert each byte to [0-255] token range
        tokens = list(txt_bytes)
        # loop text compression using BPE algorithm
        while len(tokens) >= 2:
            bp_freqs = bpe_freqs(tokens)
            # retrieve the byte pair with the lowest merge index
            # because higher index tokens might depend on lower index merged tokens
            pair = min(bp_freqs, key=lambda bp: self.enc_merges.get(bp, float("inf")))
            # no merge mapping for current target bp
            if pair not in self.enc_merges:
                break
            # merge target pair
            tokens = bpe_merge(tokens, pair, self.enc_merges[pair])

        return tokens

    def decode(self, tokens: list[Token]) -> str:
        """Decode a sequence of tokens back into text."""
        # token stream -> byte stream
        txt_bytes = b"".join(self.dec_vocab[tok] for tok in tokens)
        # byte stream -> python string
        return txt_bytes.decode("utf-8", errors="replace")


def main() -> None:
    """Load and preprocess sci-fi books dataset for tokenizer training."""
    # preprocessing
    ds = load_dataset("stevez80/Sci-Fi-Books-gutenberg", split="train")
    tokens = list("".join(ds[:100]["text"]).encode("utf-8"))
    vocab_size = 280

    # train tokenizer on dataset
    btok = BasicTokenizer()
    btok.train(tokens, vocab_size, verbose=True)

    # save vocabulary
    btok.save("token-map")

    # test
    # pay attention to how the emojis are rendered
    # emojis are 4 byte representations
    # regex pattern required pre-tokenization to ensure
    # multi bytes are not split
    tc = """
    CafÃ© naÃ¯ve rÃ©sumÃ© coÃ¶perate â€” ï¬ancÃ©e; SÃ£o Paulo vs. MÃ¼nchen.
    Î•Î»Î»Î·Î½Î¹ÎºÎ¬: Î±Î»Ï†Î¬Î²Î·Ï„Î¿, Î¼Î±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ¬ âˆ‘âˆ«âˆš â‰ˆ â‰  â‰¤ â‰¥.
    Ğ ÑƒÑÑĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚: ÑÑŠĞµÑˆÑŒ ĞµÑ‰Ñ‘ ÑÑ‚Ğ¸Ñ… Ğ¼ÑĞ³ĞºĞ¸Ñ… Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ñ… Ğ±ÑƒĞ»Ğ¾Ğº.
    Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¬Ù…ÙŠÙ„Ø©ÙŒØŒ ÙˆØ§Ù„ØªØ´ÙƒÙŠÙ„Ù Ù…Ù‡Ù…Ù‘ÙŒ.
    ×¢×‘×¨×™×ª: ×©×œ×•× ×¢×•×œ×.
    à¤¹à¤¿à¤¨à¥à¤¦à¥€: à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆà¥¤
    à¦¬à¦¾à¦‚à¦²à¦¾: à¦à¦Ÿà¦¿ à¦à¦•à¦Ÿà¦¿ à¦ªà¦°à§€à¦•à§à¦·à¦¾ à¦¬à¦¾à¦•à§à¦¯à¥¤
    í•œêµ­ì–´: ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.
    æ—¥æœ¬èª: æ—¥æœ¬èªã®æ–‡ç« ã§ã™ã€‚ã‹ãªã‚«ãƒŠæ¼¢å­—ã€‚
    ä¸­æ–‡: ç®€ä½“ä¸­æ–‡å’Œç¹é«”ä¸­æ–‡æ¸¬è©¦ã€‚
    Emoji: ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ‘©ğŸ½â€ğŸ’»ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ğŸ‡¦ğŸ‡ºğŸ‡§ğŸ‡©â¤ï¸â€ğŸ”¥
    Combining: aÌ eÌ iÌˆ oÌ„ uÌŠ (NFD-like) vs Ã¡ Ã© Ã¯ Å Å¯ (NFC).
    Zero-width: Aâ€‹Bâ€‹C (ZWSP), wordâ joinerâ test.
    Spaces: space NBSPâ€ƒEMâ€ENâ€‰THINâ€ŠHAIRâ€‚IDEOGRAPHICã€€END
    """
    tokens = btok.encode(tc)
    print(f"Total characters: {len(tc)}")
    print(f"Total tokens: {len(tokens)}")
    print(f"Tokens: {tokens}")

    dec_txt = btok.decode(tokens)
    print(f"Decoded text: {dec_txt}")


if __name__ == "__main__":
    main()
